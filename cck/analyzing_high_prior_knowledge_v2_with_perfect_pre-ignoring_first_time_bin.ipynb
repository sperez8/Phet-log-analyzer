{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sperez8\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\numpy\\lib\\function_base.py:4011: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "from functions import *\n",
    "from mining_functions import *\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "np.set_printoptions(precision=2)\n",
    "pd.set_option('precision', 2)\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = '/Google Drive/Jonathan Sarah Ido folder/data/CCK/'\n",
    "def get_path(path = PATH):\n",
    "    if os.name == 'posix':\n",
    "        path = '/Documents/code/Phet-log-analyzer/cck/raw_data_parsing_check/'\n",
    "        return os.environ['HOME']+path #'/Google Drive/Jonathan Sarah Ido folder/data/CCK/'\n",
    "    elif os.name == 'nt':\n",
    "        if os.getenv(\"COMPUTERNAME\") == 'PYRRHA':\n",
    "                path = '/Documents/git/Phet-log-analyzer/cck/raw_data_parsing_check/'\n",
    "        else:\n",
    "            path = '/git/Phet-log-analyzer/cck/raw_data_parsing_check/'\n",
    "        return os.environ['USERPROFILE']+ path.replace('/','\\\\') #'\\\\Google Drive\\Jonathan Sarah Ido folder\\data\\CCK\\\\'\n",
    "    else:\n",
    "        raise Exception('OS not recongnized. I\\'m confused.')\n",
    "        \n",
    "df = pd.read_csv(get_path() + 'phet_cck_user_actions+sophistication_WITHPAUSE_more_circuit_info_with_perfect_pre.txt',index_col=False)\n",
    "# dfx = pd.read_csv('C:\\Users\\Sarah\\Documents\\git\\Phet-log-analyzer\\cck\\\\raw_data_parsing_check\\phet_cck_user_actions+sophistication_WITHPAUSE_more_circuit_info.txt',index_col=False)\n",
    "df[\"student\"] = df[\"student\"].astype('category')\n",
    "df[\"Family\"]=df[\"Family\"].str.capitalize()\n",
    "df[\"Family_tool\"]=df[\"Family_tool\"].str.capitalize()\n",
    "df[\"Family_default\"]=df[\"Family_default\"].str.capitalize()\n",
    "df[\"Family_both\"]=df[\"Family_both\"].str.capitalize()\n",
    "\n",
    "df_scores = pd.read_csv(data_path + 'MATCHING_phet_cck_user_data_anonymized_with_perfect_pre.txt')\n",
    "df_scores[\"student\"] = df_scores[\"student\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['student']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Crop activity data to get rid of activity #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_time = 25*60*1000\n",
    "max_times = {s:min(df[df['student']==s][\"Time Stamp\"])+max_time for s in set(df['student'])}\n",
    "\n",
    "def keep_by_time (row):\n",
    "    if row[\"Time Stamp\"] <= max_times[row[\"student\"]]:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "df['keep'] = df.apply (lambda row: keep_by_time (row),axis=1)\n",
    "df=df[df['keep']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions from the notebook \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_text(attribute,family_category,cut_off, shortest_seq_length, longest_seq_length,B):\n",
    "    text = \"\"\"Showing sequences for students split by {0}, using the categories {1}.\n",
    "            Removed sequences used by less than {2}% of students.\n",
    "            Found sequences of lenght {3} to {4}.\n",
    "            Using {5} time bins\"\"\".format(attribute,family_category,int(cut_off*100), shortest_seq_length, longest_seq_length,B)\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = plt.gca()\n",
    "    ax.text(0.5,0.5,text,\n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='center',\n",
    "        fontsize = 15)\n",
    "    plt.axis('off')\n",
    "    return fig\n",
    "\n",
    "def rank_sequences(sequence_counts,B,axesnum=None,ignore_first_time_bin=False):\n",
    "    ranks = []\n",
    "    for seq,counts in sequence_counts.iteritems():\n",
    "#         if np.sum(counts)>0:\n",
    "        ranks.append((seq,calc_infogain(counts,B,axesnum,ignore_first_time_bin)))\n",
    "    return sorted(ranks, key=lambda tup: tup[1])\n",
    "\n",
    "def get_top_seqs(ranks,N):\n",
    "    return ranks[-N:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our groups\n",
    "So we want to take a look at what the high prior knowledge are doing. Remember that we have 25 such students, 22 HH and 3 HL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "median_learning0 = 0.45\n",
    "df_scores['split pre'] = df_scores.apply (lambda row: label_learning (median_learning0,row,\"z pre\"),axis=1)\n",
    "median_learning2 = np.median(df_scores[df_scores['split pre']=='low']['z post t2'])\n",
    "df_scores['split post t2'] = df_scores.apply (lambda row: label_learning (median_learning2,row,\"z post t2\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high high 22\n",
      "high low 3\n",
      "low high 38\n",
      "low low 36\n"
     ]
    }
   ],
   "source": [
    "for x in ['high','low']:\n",
    "    for y in ['high','low']:\n",
    "        print x,y, len(set(df_scores[(df_scores['split pre']==x)&(df_scores['split post t2']==y)]['student']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare the three groups to each other. We would like to know if:\n",
    "- HH look like LL in which case LL use experts strategies that are productive with certain level of prior knowledge\n",
    "- HH look like LH in which case they used productive strategies\n",
    "\n",
    "We need to make a new funciton to run the sequence mining with 3 groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make those three groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_3_groups (row):\n",
    "    if row['split pre']  == 'high' and row['split post t2'] == 'high':\n",
    "        return 'HH'\n",
    "    if row['split pre']  == 'high' and row['split post t2'] == 'low':\n",
    "        return 'HL'\n",
    "    if row['split pre']  == 'low' and row['split post t2'] == 'high':\n",
    "        return 'LH'\n",
    "    if row['split pre']  == 'low' and row['split post t2'] == 'low':\n",
    "        return 'LL'\n",
    "df_scores['three groups'] = df_scores.apply (lambda row: label_3_groups (row),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Let's run the sequence mining like we usually do on all 3 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sequence use over 1 time bins for 96 students split by three groups. \n",
      "    Keeping only sequences used once by at least 40% of students \n",
      "    in each group and overall.\n",
      "Getting sequence use over 3 time bins for 96 students split by three groups. \n",
      "    Keeping only sequences used once by at least 40% of students \n",
      "    in each group and overall.\n"
     ]
    }
   ],
   "source": [
    "# students = list(set(get_students()) - set(df_scores[df_scores['three groups']=='HL']['student']))\n",
    "students = list(set(df_scores['student']) - set(df_scores[df_scores['three groups']=='HL']['student']))\n",
    "CUT_OFF = 0.40 #we keep only sequences used once by at least 10% of students\n",
    "shortest_seq_length = 1\n",
    "longest_seq_length = 10\n",
    "\n",
    "### PARAMETERS\n",
    "attribute = 'three groups'\n",
    "level1 = 'LH'\n",
    "level2 = 'LL'\n",
    "level3 = 'HH'\n",
    "family_category = \"Family\"\n",
    "group_sizes = np.array([len(set(df_scores[df_scores['three groups']==level1]['student'])),len(set(df_scores[df_scores['three groups']==level2]['student'])),len(set(df_scores[df_scores['three groups']==level3]['student']))])\n",
    "\n",
    "BINS = 1\n",
    "axis = 1\n",
    "sequence_counts_group = get_sequence_use_by_timebin(df,students,family_category,\n",
    "                                          BINS,attribute,level1,level2,\n",
    "                                          shortest_seq_length,longest_seq_length,CUT_OFF,level3='HH',ignore=['I'])\n",
    "sequence_counts_group = {s:(counts.T/group_sizes).T for s,counts in sequence_counts_group.iteritems()}\n",
    "tops_group = get_top_seqs(rank_sequences(sequence_counts_group,BINS,axis),15)\n",
    "\n",
    "BINS = 3\n",
    "axis = None\n",
    "sequence_counts_both = get_sequence_use_by_timebin(df,students,family_category,\n",
    "                                  BINS,attribute,level1,level2,\n",
    "                                  shortest_seq_length,longest_seq_length,CUT_OFF,level3='HH',ignore=['I'])\n",
    "sequence_counts_both = {s:counts/group_sizes[:,None] for s,counts in sequence_counts_both.iteritems()}\n",
    "tops_both = get_top_seqs(rank_sequences(sequence_counts_both,BINS,axis),15)\n",
    "\n",
    "axis = 0\n",
    "tops_time = get_top_seqs(rank_sequences(sequence_counts_both,BINS,axis),15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  ----  --  --  --  ------  -----  ------  ------  ------\n",
      "SEQ      IG    LH  LL  HH  chi^2   p-val  LH-res  LL-res  HH-res\n",
      "PTcCTc   0.15  47  33  13  7.056   0.029  2.17    -0.17   -2.33\n",
      "CTsCTs   0.15  26  44  13  6.599   0.037  -0.67   2.35    -1.93\n",
      "PCTbP    0.11  21  16  40  4.696   0.096  -0.54   -1.3    2.12\n",
      "PTsPC    0.11  52  19  45  9.172   0.01   2.3     -2.98   0.76\n",
      "TbPCP    0.1   23  16  40  4.338   0.114  -0.24   -1.46   1.96\n",
      "PTsP     0.09  60  25  54  10.271  0.006  2.34    -3.17   0.93\n",
      "TcCPC    0.09  21  50  31  6.95    0.031  -2.22   2.5     -0.29\n",
      "CPCTs    0.08  26  41  18  4.024   0.134  -0.67   1.89    -1.4\n",
      "CPTs     0.08  39  19  45  5.224   0.073  1.03    -2.24   1.37\n",
      "CPCPCPC  0.08  18  38  40  4.844   0.089  -2.2    1.25    1.11\n",
      "CPCTcC   0.07  28  52  27  5.753   0.056  -1.4    2.4     -1.13\n",
      "PTcC     0.06  52  55  27  4.937   0.085  0.75    1.16    -2.21\n",
      "CTcCTcP  0.06  39  47  22  3.482   0.175  0.15    1.35    -1.74\n",
      "CTsPC    0.06  44  44  22  3.391   0.183  0.84    0.75    -1.84\n",
      "CPCPCP   0.06  26  52  50  6.141   0.046  -2.47   1.71    0.9\n",
      "-------  ----  --  --  --  ------  -----  ------  ------  ------\n"
     ]
    }
   ],
   "source": [
    "table = [['SEQ','IG','LH','LL','HH','chi^2','p-val', 'LH-res', \"LL-res\", 'HH-res']]\n",
    "tops_group.sort(key=lambda x: -x[1])\n",
    "for seq,ig in tops_group:\n",
    "#     print seq, round(ig,3), np.ndarray.tolist(sequence_counts_group[seq].T*group_sizes)[0]\n",
    "    lh,ll,hh = (sequence_counts_group[seq].T*group_sizes)[0]\n",
    "    tb = [[lh, group_sizes[0]-lh], [ll, group_sizes[1]-ll], [hh, group_sizes[2]-hh]]\n",
    "    chi2, p, dof, ex = stats.chi2_contingency(tb, correction=False)\n",
    "    res = (tb - ex)/np.sqrt(ex)\n",
    "    cmarg = np.array(tb).sum(axis=0)\n",
    "    rmarg = np.array(tb).sum(axis=1)\n",
    "    n = np.array(tb).sum()\n",
    "    res_adj = res\n",
    "    res_adj[:,0] = res_adj[:,0]/np.sqrt(1-rmarg/n)/np.sqrt(1-cmarg[0]/n)\n",
    "    row = [seq, str(round(ig,2))] + [int(x*100) for x in list(np.ndarray.tolist(sequence_counts_group[seq].T)[0])] + [round(chi2, 3), round(p, 3)] + [round(r,2) for r in res_adj[:,0]]\n",
    "    table.append(row)\n",
    "print tabulate(table)\n",
    "np.savetxt(\"by_group_threshold30.csv\", table, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  ----  --  --  --\n",
      "SEQ      IG    t1  t2  t3\n",
      "TbCP     0.74  32  3   4\n",
      "PTb      0.72  40  6   3\n",
      "CTbP     0.49  42  9   7\n",
      "TbPC     0.43  42  9   9\n",
      "CTbPC    0.4   31  8   6\n",
      "CTsPTs   0.4   29  16  2\n",
      "TbP      0.38  50  11  12\n",
      "PTsPC    0.38  28  11  4\n",
      "PCTbP    0.37  16  3   6\n",
      "PCTsP    0.36  26  20  2\n",
      "CPCTs    0.32  23  9   5\n",
      "TbPCP    0.3   18  6   4\n",
      "CTcPTcP  0.3   11  25  4\n",
      "TcPTcPC  0.29  8   27  7\n",
      "CTbC     0.28  42  11  14\n",
      "-------  ----  --  --  --\n"
     ]
    }
   ],
   "source": [
    "table = [['SEQ','IG','t1','t2','t3', 't4']]\n",
    "tops_time.sort(key=lambda x: -x[1])\n",
    "for seq,ig in tops_time:\n",
    "#     print seq, round(ig,3), np.ndarray.tolist(sequence_counts_time[seq].T*group_sizes)[0]\n",
    "    row = [seq,str(round(ig,2))] + [int(x*100) for x in np.sum((sequence_counts_both[seq].T*group_sizes).T, axis=0)/sum(group_sizes)]\n",
    "    table.append(row)\n",
    "print tabulate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  ----  --  --  --  --  --  --\n",
      "SEQ      IG    LH  LL  HH  t1  t2  t3\n",
      "TbCP     0.82  47  33  36  32  3   4\n",
      "PTb      0.81  52  38  63  40  6   3\n",
      "CPCTs    0.57  36  52  18  23  9   5\n",
      "PTsPC    0.56  57  22  54  28  11  4\n",
      "PCTbP    0.56  21  19  45  16  3   6\n",
      "CTbP     0.54  50  58  77  42  9   7\n",
      "TbPC     0.46  63  55  68  42  9   9\n",
      "CTsPTs   0.45  52  38  54  29  16  2\n",
      "CTbPC    0.44  39  44  59  31  8   6\n",
      "PCTsP    0.44  63  36  45  26  20  2\n",
      "TbP      0.42  71  66  90  50  11  12\n",
      "TbPCP    0.38  28  22  40  18  6   4\n",
      "CTsCTs   0.37  34  55  18  18  14  5\n",
      "CTcPTcP  0.36  47  41  27  11  25  4\n",
      "PTsP     0.36  73  27  68  31  16  7\n",
      "-------  ----  --  --  --  --  --  --\n"
     ]
    }
   ],
   "source": [
    "table = [['SEQ','IG','LH','LL','HH','t1','t2','t3','t4']]\n",
    "tops_both.sort(key=lambda x: -x[1])\n",
    "for seq,ig in tops_both:\n",
    "#     print seq, round(ig,3), np.ndarray.tolist(sequence_counts_both[seq].T*group_sizes)[0]\n",
    "    row = [seq,str(round(ig,2))] + [int(x*100) for x in np.sum(sequence_counts_both[seq], axis=1)] + [int(x*100) for x in np.sum((sequence_counts_both[seq].T*group_sizes).T, axis=0)/sum(group_sizes)]\n",
    "    table.append(row)\n",
    "print tabulate(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ignore first time bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequencies_without_first_time_bin(frequencies_by_bin,students_in_group):\n",
    "    '''\n",
    "    Takes frequencies of sequencies by time bins and removes the first time bin.\n",
    "    Then sums the frequency per sequence per student overall.\n",
    "    Return this information for only relevant students\n",
    "    \n",
    "    Arguments:\n",
    "        frequencies_by_bin = {student: [list of Counters for each bin]}\n",
    "                           = {student1: [ Counter{'TPT':3, 'CPT':5...}, Counter{},... ],  ...}\n",
    "        students_in_group = list of students for whom to return this data\n",
    "        \n",
    "    returns:\n",
    "        {student1: Counter{'TPT':5, ...},  ...}\n",
    "    '''\n",
    "    return {student:sum(bins[1:], Counter()) for student,bins in frequencies_by_bin.iteritems() if student in students_in_group}\n",
    "\n",
    "\n",
    "def get_sequence_use_by_timebin(df, students, category_column, B, attribute,\n",
    "            level1, level2, shortest_seq_length, longest_seq_length, cut_off,level3 = None,\n",
    "            remove_actions = [],ignore=['I'],ignore_first_time_bin=False):\n",
    "    '''\n",
    "    '''\n",
    "    \n",
    "    print \"\"\"Getting sequence use over {3} time bins for {0} students split by {1}. \n",
    "    Keeping only sequences used once by at least {2}% of students \n",
    "    in each group and overall.\"\"\".format(len(students),attribute,int(cut_off*100),B)\n",
    "\n",
    "    #get all seqs per student per time bin        \n",
    "    blocks, time_coords =  get_blocks_withTime_new(df, students, category_column, start=False, ignore=ignore, remove_actions = remove_actions)\n",
    "    frequencies_by_bin = get_frequencies_by_bin(blocks, students, time_coords, B, shortest = shortest_seq_length, longest = longest_seq_length)\n",
    "\n",
    "    cleaned_frequencies = Counter()\n",
    "    levels = [level1,level2]\n",
    "    if level3: levels.append(level3)\n",
    "    for level in levels:\n",
    "        students_in_group = get_students(attribute,level)\n",
    "        N = int(math.ceil(cut_off*len(students_in_group)))\n",
    "#         print level\n",
    "        \n",
    "#         students_in_group = [19930112]\n",
    "        blocks, time_coords =  get_blocks_withTime_new(df, students_in_group, category_column, start=False, ignore=ignore, remove_actions = remove_actions)\n",
    "        #find all sequences to consider for analysis, given that they have been used by enough students\n",
    "        if ignore_first_time_bin:\n",
    "            frequencies =  get_frequencies_without_first_time_bin(frequencies_by_bin,students_in_group)\n",
    "        else:\n",
    "            frequencies = get_frequencies(blocks, shortest = shortest_seq_length, longest = longest_seq_length)\n",
    "#         frequencies[19930112]['C']=100\n",
    "#         print frequencies[19930112]\n",
    "        ## number of students who used freq Counter{'TPT':5,...}\n",
    "        counts_frequencies = Counter({f:sum([ 1 if f in freq else 0 for freq in frequencies.values()]) for f in list(sum(frequencies.values(),Counter()))})\n",
    "        cleaned_frequencies += remove_rare_frequencies(counts_frequencies, N)    \n",
    "    \n",
    "    counts = count_use_per_group_per_bin(cleaned_frequencies, frequencies_by_bin, B, attribute, level1, level2, level3 = level3)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sequence use over 3 time bins for 1 students split by three groups. \n",
      "    Keeping only sequences used once by at least 40% of students \n",
      "    in each group and overall.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sequence_use_by_timebin(df,students[0:1],family_category,\n",
    "                                  BINS,attribute,level1,level2,\n",
    "                                  shortest_seq_length,longest_seq_length,CUT_OFF,\n",
    "                                    level3='HH',ignore=['I'],ignore_first_time_bin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sequence use over 3 time bins for 1 students split by three groups. \n",
      "    Keeping only sequences used once by at least 40% of students \n",
      "    in each group and overall.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'CP': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'CPC': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'CPCP': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'CPCPC': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'CPCPCP': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'CPCPCPC': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'CPCTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CPCTcC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CPCTs': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CPTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CPTs': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTb': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTbC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTbP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTbPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTcC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTcCP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTcCTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTcCTcC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTcCTcP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTcP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTcPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTcPTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTcPTcP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTs': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTsC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTsCTs': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTsP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTsPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'CTsPTs': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'P': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'PC': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'PCP': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'PCPC': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'PCPCP': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'PCPCPC': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'PCPCPCP': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.]]), 'PCPCTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PCTb': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PCTbP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PCTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PCTcC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PCTcCTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PCTcP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PCTs': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PCTsC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PCTsP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PTb': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PTcC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PTcCTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PTcP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PTcPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PTs': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PTsP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'PTsPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'Tb': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TbC': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TbCP': array([[ 0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TbP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TbPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TbPCP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'Tc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcCP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcCPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcCTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcCTcC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcCTcCTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcCTcP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcCTcPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcPCP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcPCPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcPCTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcPTc': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcPTcP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TcPTcPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'Ts': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TsC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TsCTs': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TsP': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TsPC': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]]), 'TsPTs': array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sequence_use_by_timebin(df,students[0:1],family_category,\n",
    "                                  BINS,attribute,level1,level2,\n",
    "                                  shortest_seq_length,longest_seq_length,CUT_OFF,\n",
    "                                    level3='HH',ignore=['I'],ignore_first_time_bin=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sequence use over 3 time bins for 96 students split by three groups. \n",
      "    Keeping only sequences used once by at least 40% of students \n",
      "    in each group and overall.\n"
     ]
    }
   ],
   "source": [
    "%aimport mining_functions\n",
    "# students = list(set(get_students()) - set(df_scores[df_scores['three groups']=='HL']['student']))\n",
    "students = list(set(df_scores['student']) - set(df_scores[df_scores['three groups']=='HL']['student']))\n",
    "CUT_OFF = 0.40 #we keep only sequences used once by at least 10% of students\n",
    "shortest_seq_length = 1\n",
    "longest_seq_length = 10\n",
    "\n",
    "### PARAMETERS\n",
    "attribute = 'three groups'\n",
    "level1 = 'LH'\n",
    "level2 = 'LL'\n",
    "level3 = 'HH'\n",
    "family_category = \"Family\"\n",
    "group_sizes = np.array([len(set(df_scores[df_scores['three groups']==level1]['student'])),len(set(df_scores[df_scores['three groups']==level2]['student'])),len(set(df_scores[df_scores['three groups']==level3]['student']))])\n",
    "\n",
    "# BINS = 1\n",
    "# axis = 1\n",
    "# sequence_counts_group = get_sequence_use_by_timebin(df,students,family_category,\n",
    "#                                           BINS,attribute,level1,level2,\n",
    "#                                           shortest_seq_length,longest_seq_length,CUT_OFF,level3='HH',ignore=['I'])\n",
    "# sequence_counts_group = {s:(counts.T/group_sizes).T for s,counts in sequence_counts_group.iteritems()}\n",
    "# tops_group = get_top_seqs(rank_sequences(sequence_counts_group,BINS,axis),15)\n",
    "\n",
    "BINS = 3\n",
    "axis = None\n",
    "sequence_counts_both_ignore_first = get_sequence_use_by_timebin(df,students,family_category,\n",
    "                                  BINS,attribute,level1,level2,\n",
    "                                  shortest_seq_length,longest_seq_length,CUT_OFF,\n",
    "                                    level3='HH',ignore=['I'],ignore_first_time_bin=True)\n",
    "sequence_counts_both_ignore_first = {s:counts/group_sizes[:,None] for s,counts in sequence_counts_both_ignore_first.iteritems()}\n",
    "tops_both_ignore_first = get_top_seqs(rank_sequences(sequence_counts_both_ignore_first,BINS,axis),15)\n",
    "\n",
    "axis = 0\n",
    "tops_time_ignore_first = get_top_seqs(rank_sequences(sequence_counts_both_ignore_first,BINS,axis,ignore_first_time_bin=True),15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cstg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  ----  --  --  --\n",
      "SEQ      IG    t1  t2  t3\n",
      "TcPTcP   0.22  10  30  9\n",
      "PTs      0.15  41  27  10\n",
      "CTcPTc   0.14  20  32  12\n",
      "TsPC     0.13  41  30  13\n",
      "TcPTc    0.11  19  44  20\n",
      "TsP      0.09  47  35  17\n",
      "TcPCTc   0.07  14  34  15\n",
      "PCTcCTc  0.07  27  19  10\n",
      "CPCTc    0.07  33  33  16\n",
      "PTcP     0.06  14  40  21\n",
      "CTcCTcC  0.06  17  21  13\n",
      "TsC      0.05  40  30  18\n",
      "PCTcP    0.05  25  33  19\n",
      "PTcPC    0.04  10  33  19\n",
      "CPTc     0.04  23  29  18\n",
      "-------  ----  --  --  --\n"
     ]
    }
   ],
   "source": [
    "table = [['SEQ','IG','t1','t2','t3', 't4']]\n",
    "tops_time_ignore_first.sort(key=lambda x: -x[1])\n",
    "for seq,ig in tops_time_ignore_first:\n",
    "#     print seq, round(ig,3), np.ndarray.tolist(sequence_counts_time[seq].T*group_sizes)[0]\n",
    "    row = [seq,str(round(ig,2))] + [int(x*100) for x in np.sum((sequence_counts_both_ignore_first[seq].T*group_sizes).T, axis=0)/sum(group_sizes)]\n",
    "    table.append(row)\n",
    "print tabulate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  ----  --  --  --  --  --  --\n",
      "SEQ      IG    LH  LL  HH  t1  t2  t3\n",
      "CTcCTcC  0.34  28  47  40  17  21  13\n",
      "TcCPC    0.32  21  50  31  10  18  20\n",
      "TcPTcP   0.31  50  41  31  10  30  9\n",
      "PCTcCTc  0.25  31  55  40  27  19  10\n",
      "CTcCTc   0.25  50  66  50  36  30  21\n",
      "CTb      0.23  71  58  81  60  23  22\n",
      "PTs      0.22  65  44  63  41  27  10\n",
      "TcPCTc   0.2   47  44  40  14  34  15\n",
      "TcPTc    0.2   63  61  40  19  44  20\n",
      "PTcP     0.19  63  50  45  14  40  21\n",
      "PTcPC    0.19  52  47  45  10  33  19\n",
      "Tb       0.19  81  63  90  68  28  27\n",
      "TsPC     0.16  68  55  54  41  30  13\n",
      "CTcPTc   0.16  52  52  36  20  32  12\n",
      "CPCPCP   0.15  26  52  50  26  17  18\n",
      "-------  ----  --  --  --  --  --  --\n"
     ]
    }
   ],
   "source": [
    "table = [['SEQ','IG','LH','LL','HH','t1','t2','t3','t4']]\n",
    "tops_both_ignore_first.sort(key=lambda x: -x[1])\n",
    "for seq,ig in tops_both_ignore_first:\n",
    "#     print seq, round(ig,3), np.ndarray.tolist(sequence_counts_both[seq].T*group_sizes)[0]\n",
    "#     print seq,sequence_counts_both_ignore_first[seq]\n",
    "    row = [seq,str(round(ig,2))] + [int(x*100) for x in np.sum(sequence_counts_group[seq], axis=1)] + [int(x*100) for x in np.sum((sequence_counts_both_ignore_first[seq].T*group_sizes).T, axis=0)/sum(group_sizes)]\n",
    "    table.append(row)\n",
    "print tabulate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "all_seqs = zip(*tops_both_ignore_first)[0]\n",
    "fig, axes = plt.subplots(len(tops_both_ignore_first),1)#, sharex=True, sharey=True)\n",
    "maximum = max([np.amax(counts) for seq,counts in sequence_counts_both_ignore_first.iteritems() if seq in all_seqs])\n",
    "for i,(seq,ig) in enumerate(tops_both_ignore_first):\n",
    "    ax = axes[i]\n",
    "    data = sequence_counts_both_ignore_first[seq]\n",
    "    hl,ll,hh = np.ndarray.tolist(sequence_counts_group[seq].T*group_sizes)[0]\n",
    "    seq = \"$\"+seq.replace('Ts','T_2').replace('Tb','T_1').replace('Tc','T_m')+\"$\"\n",
    "    ax.text(0,3.05, seq, fontsize=14)\n",
    "    ax.text(2.0,3.05, \"$IG$= {0}, $LH$: {1}, $LL$: {2}, $HH$: {3}\".format(str(round(ig,2)),hl,ll,hh), fontsize=10)\n",
    "    \n",
    "    heatmap = sns.heatmap(data, ax=ax, cmap=plt.cm.Blues,alpha=0.8, vmin=0, vmax=maximum, cbar=False,\n",
    "                          annot=True) #, annot_kws={'fontweight':'bold'})\n",
    "    fig.set_size_inches(10,40)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5)\n",
    "    ylabels = ['$HH$','$LL$','$LH$']\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    ax.set_xticklabels(['','','',''])\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific sequence look-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_actions = [('Tb',0),('Ts',0),('Tc',0),('P',0),('C',0)]\n",
    "table = [['SEQ','IG','LH','LL','HH','chi^2','p-val', 'LH-res', \"LL-res\", 'HH-res']]\n",
    "\n",
    "seq = 'Tc'\n",
    "lh,ll,hh = (sequence_counts_group[seq].T*group_sizes)[0]\n",
    "tb = [[lh, group_sizes[0]-lh], [ll, group_sizes[1]-ll], [hh, group_sizes[2]-hh]]\n",
    "chi2, p, dof, ex = stats.chi2_contingency(tb, correction=False)\n",
    "res = (tb - ex)/np.sqrt(ex)\n",
    "cmarg = np.array(tb).sum(axis=0)\n",
    "rmarg = np.array(tb).sum(axis=1)\n",
    "res_adj = res\n",
    "res_adj[:,0] = res_adj[:,0]/np.sqrt(1-rmarg/n)/np.sqrt(1-cmarg[0]/n)\n",
    "n = np.array(tb).sum()\n",
    "row = [seq, str(round(ig,2))] + [int(x*100) for x in list(np.ndarray.tolist(sequence_counts_group[seq].T)[0])] + [round(chi2, 3), round(p, 3)] + [round(r,2) for r in res_adj[:,0]]\n",
    "table.append(row)\n",
    "print tabulate(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_actions = [('Tb',0),('Ts',0),('Tc',0),('P',0),('C',0),]\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "all_seqs = zip(*all_actions)[0]\n",
    "fig, axes = plt.subplots(len(all_actions),1)#, sharex=True, sharey=True)\n",
    "maximum = max([np.amax(counts) for seq,counts in sequence_counts_both.iteritems() if seq in all_seqs])\n",
    "\n",
    "for i,(seq,ig) in enumerate(all_actions):\n",
    "    ax = axes[i]\n",
    "    data = sequence_counts_both[seq]\n",
    "    ax = axes[i]\n",
    "    hl,ll,h_ = np.ndarray.tolist(sequence_counts_group[seq].T*group_sizes)[0]\n",
    "    seq = \"$\"+seq.replace('Ts','T_2').replace('Tb','T_1').replace('Tc','T_m')+\"$\"\n",
    "    ax.text(0,3.05, seq, fontsize=14)\n",
    "    ax.text(2.0,3.05, \"$IG$= {0}, $LH$: {1}, $LL$: {2}, $HH$: {3}\".format(str(round(ig,2)),hl,ll,h_), fontsize=10)\n",
    "    \n",
    "    heatmap = sns.heatmap(data, ax=ax, cmap=plt.cm.Blues,alpha=0.8, vmin=0, vmax=maximum, cbar=False,\n",
    "                          annot=True) #, annot_kws={'fontweight':'bold'})\n",
    "    fig.set_size_inches(10,12)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5)\n",
    "    ylabels = ['$HH$','$LL$','$LH$']\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    ax.set_xticklabels(['','','',''])\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's analyze the building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blocks_LH, time_coords_LH =  get_blocks_withTime_new(df,get_students('three groups','LH'),\"Family\",start=False,ignore=['I'], as_list=True)\n",
    "blocks_LL, time_coords_LL =  get_blocks_withTime_new(df,get_students('three groups','LL'),\"Family\",start=False,ignore=['I'], as_list=True)\n",
    "blocks_HH, time_coords_HH =  get_blocks_withTime_new(df,get_students('three groups','HH'),\"Family\",start=False,ignore=['I'], as_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequencies(seq_of_interest,blocks1,blocks2,blocks3,normalize_by='length'):\n",
    "    def coo(seq, seq_of_interest):\n",
    "        return float(occurrences(''.join(seq),seq_of_interest))\n",
    "    \n",
    "    if normalize_by == None:\n",
    "        # count number of occurrences of \"seq_of_interest\" in a student's seq\n",
    "        freq1 = {student : coo(seq, seq_of_interest) for student,seq in blocks1.iteritems()}\n",
    "        freq2 = {student : coo(seq, seq_of_interest) for student,seq in blocks2.iteritems()}\n",
    "        freq3 = {student : coo(seq, seq_of_interest) for student,seq in blocks3.iteritems()}\n",
    "    elif normalize_by == 'length':\n",
    "        len_seqi = len(''.join([c for c in seq_of_interest if c.isupper()]))\n",
    "        # count number of occurrences of \"seq_of_interest\" in a student's seq divided by the length of seq\n",
    "        freq1 = {student : coo(seq, seq_of_interest)/(len(seq)-len_seqi-1) if coo(seq, seq_of_interest) >0 else 0 for student,seq in blocks1.iteritems()}\n",
    "        freq2 = {student : coo(seq, seq_of_interest)/(len(seq)-len_seqi-1) if coo(seq, seq_of_interest) >0 else 0 for student,seq in blocks2.iteritems()}\n",
    "        freq3 = {student : coo(seq, seq_of_interest)/(len(seq)-len_seqi-1) if coo(seq, seq_of_interest) >0 else 0 for student,seq in blocks3.iteritems()}\n",
    "    else:\n",
    "        # count number of occurrences of \"seq_of_interest\" in a student's seq divided by the number of occurrences of the seq \"normalize_by\"\n",
    "        freq1 = {student : coo(seq, seq_of_interest)/coo(seq, normalize_by) if coo(seq, normalize_by) >0 else 0 for student,seq in blocks1.iteritems()}\n",
    "        freq2 = {student : coo(seq, seq_of_interest)/coo(seq, normalize_by) if coo(seq, normalize_by) >0 else 0 for student,seq in blocks2.iteritems()}\n",
    "        freq3 = {student : coo(seq, seq_of_interest)/coo(seq, normalize_by) if coo(seq, normalize_by) >0 else 0 for student,seq in blocks3.iteritems()}\n",
    "    return freq1.values(),freq2.values(),freq3.values()\n",
    "\n",
    "def occurrences(string, sub):\n",
    "    count = start = 0\n",
    "    while True:\n",
    "        start = string.find(sub, start) + 1\n",
    "        if start > 0:\n",
    "            count+=1\n",
    "        else:\n",
    "            return count\n",
    "        \n",
    "def make_hist_continuous(seq_of_interest,freq_LH,freq_LL,freq_HH):\n",
    "    fig, axs = plt.subplots(1,figsize=(12,2.5))\n",
    "    alldata = np.concatenate((freq_LH, freq_LL, freq_HH), axis=0)\n",
    "    binsize = max(alldata)/10.\n",
    "    bins = np.arange(0,max(alldata)+binsize,binsize)\n",
    "#     plt.hist(freq_HL,color ='r',alpha = 0.4,bins = bins)\n",
    "#     plt.hist(freq_LL,color = 'b',alpha = 0.4,bins = bins)\n",
    "#     plt.hist(freq_HH,color = 'g',alpha = 0.4,bins = bins)\n",
    "    plt.hist([freq_LH,freq_LL,freq_HH], bins=bins, stacked=True)\n",
    "    plt.title(\"Use of the sequence ''{0}'' by LH (red) and LL (blue) and HH (purple)\".format(seq_of_interest))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq = 'Tb'\n",
    "freq_LH,freq_LL,freq_HH = get_frequencies(seq,blocks_LH,blocks_LL,blocks_HH,normalize_by=None)\n",
    "plot = make_hist_continuous(seq,freq_LH,freq_LL,freq_HH)\n",
    "# plot.show()\n",
    "# perform_comparative_stats(seq,freq_HL,freq_LL)\n",
    "# freq_HL,freq_LL = get_frequencies(seq,blocks_merged_HL,blocks_merged_LL,normalize_by='P')\n",
    "# plot = make_hist_continuous(seq,freq_HL,freq_LL)\n",
    "# plot.show()\n",
    "# perform_comparative_stats(seq,freq_HL,freq_LL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.libqsturng import psturng\n",
    "import warnings\n",
    "\n",
    "\n",
    "def kw_dunn(groups, to_compare=None, alpha=0.05, method='bonf'):\n",
    "    \"\"\"\n",
    "\n",
    "    Kruskal-Wallis 1-way ANOVA with Dunn's multiple comparison test\n",
    "\n",
    "    Arguments:\n",
    "    ---------------\n",
    "    groups: sequence\n",
    "        arrays corresponding to k mutually independent samples from\n",
    "        continuous populations\n",
    "\n",
    "    to_compare: sequence\n",
    "        tuples specifying the indices of pairs of groups to compare, e.g.\n",
    "        [(0, 1), (0, 2)] would compare group 0 with 1 & 2. by default, all\n",
    "        possible pairwise comparisons between groups are performed.\n",
    "\n",
    "    alpha: float\n",
    "        family-wise error rate used for correcting for multiple comparisons\n",
    "        (see statsmodels.stats.multitest.multipletests for details)\n",
    "\n",
    "    method: string\n",
    "        method used to adjust p-values to account for multiple corrections (see\n",
    "        statsmodels.stats.multitest.multipletests for options)\n",
    "\n",
    "    Returns:\n",
    "    ---------------\n",
    "    H: float\n",
    "        Kruskal-Wallis H-statistic\n",
    "\n",
    "    p_omnibus: float\n",
    "        p-value corresponding to the global null hypothesis that the medians of\n",
    "        the groups are all equal\n",
    "\n",
    "    Z_pairs: float array\n",
    "        Z-scores computed for the absolute difference in mean ranks for each\n",
    "        pairwise comparison\n",
    "\n",
    "    p_corrected: float array\n",
    "        corrected p-values for each pairwise comparison, corresponding to the\n",
    "        null hypothesis that the pair of groups has equal medians. note that\n",
    "        these are only meaningful if the global null hypothesis is rejected.\n",
    "\n",
    "    reject: bool array\n",
    "        True for pairs where the null hypothesis can be rejected for the given\n",
    "        alpha\n",
    "\n",
    "    Reference:\n",
    "    ---------------\n",
    "    Gibbons, J. D., & Chakraborti, S. (2011). Nonparametric Statistical\n",
    "    Inference (5th ed., pp. 353-357). Boca Raton, FL: Chapman & Hall.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # omnibus test (K-W ANOVA)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    groups = [np.array(gg) for gg in groups]\n",
    "\n",
    "    k = len(groups)\n",
    "\n",
    "    n = np.array([len(gg) for gg in groups])\n",
    "    if np.any(n < 5):\n",
    "        warnings.warn(\"Sample sizes < 5 are not recommended (K-W test assumes \"\n",
    "                      \"a chi square distribution)\")\n",
    "\n",
    "    allgroups = np.concatenate(groups)\n",
    "    N = len(allgroups)\n",
    "    ranked = stats.rankdata(allgroups)\n",
    "\n",
    "    # correction factor for ties\n",
    "    T = stats.tiecorrect(ranked)\n",
    "    if T == 0:\n",
    "        raise ValueError('All numbers are identical in kruskal')\n",
    "\n",
    "    # sum of ranks for each group\n",
    "    j = np.insert(np.cumsum(n), 0, 0)\n",
    "    R = np.empty(k, dtype=np.float)\n",
    "    for ii in range(k):\n",
    "        R[ii] = ranked[j[ii]:j[ii + 1]].sum()\n",
    "\n",
    "    # the Kruskal-Wallis H-statistic\n",
    "    H = (12. / (N * (N + 1.))) * ((R ** 2.) / n).sum() - 3 * (N + 1)\n",
    "\n",
    "    # apply correction factor for ties\n",
    "    H /= T\n",
    "\n",
    "    df_omnibus = k - 1\n",
    "    p_omnibus = stats.chisqprob(H, df_omnibus)\n",
    "\n",
    "    # multiple comparisons\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # by default we compare every possible pair of groups\n",
    "    if to_compare is None:\n",
    "        to_compare = tuple(combinations(range(k), 2))\n",
    "\n",
    "    ncomp = len(to_compare)\n",
    "\n",
    "    Z_pairs = np.empty(ncomp, dtype=np.float)\n",
    "    p_uncorrected = np.empty(ncomp, dtype=np.float)\n",
    "    Rmean = R / n\n",
    "\n",
    "    for pp, (ii, jj) in enumerate(to_compare):\n",
    "        \n",
    "        # standardized score\n",
    "        ts3_ts = list(np.unique(allgroups, return_counts=True)[1])\n",
    "        E_ts3_ts = sum([x**3 - x for x in ts3_ts if x>1])\n",
    "\n",
    "        if sum([x>1 for x in ts3_ts]) > 0:\n",
    "            warnings.warn(\"We see ties.\")\n",
    "\n",
    "            yi = np.abs(Rmean[ii] - Rmean[jj])\n",
    "            theta10 = (N * (N + 1)) / 12\n",
    "            theta11 =  E_ts3_ts / ( 12* (N - 1) )\n",
    "            theta2 = (1. / n[ii] + 1. / n[jj])\n",
    "            theta = np.sqrt( (theta10 - theta11) * theta2 )\n",
    "            Zij = yi / theta\n",
    "        else:\n",
    "            Zij = (np.abs(Rmean[ii] - Rmean[jj]) /\n",
    "                   np.sqrt((1. / 12.) * N * (N + 1) * (1. / n[ii] + 1. / n[jj])))\n",
    "        \n",
    "#         # standardized score\n",
    "#         Zij = (np.abs(Rmean[ii] - Rmean[jj]) /\n",
    "#                np.sqrt((1. / 12.) * N * (N + 1) * (1. / n[ii] + 1. / n[jj])))\n",
    "        Z_pairs[pp] = Zij\n",
    "\n",
    "    # corresponding p-values obtained from upper quantiles of the standard\n",
    "    # normal distribution\n",
    "    p_uncorrected = stats.norm.sf(Z_pairs) * 2.\n",
    "\n",
    "    # correction for multiple comparisons\n",
    "    reject, p_corrected, alphac_sidak, alphac_bonf = multipletests(\n",
    "        p_uncorrected, method=method\n",
    "    )\n",
    "\n",
    "    return H, p_omnibus, Z_pairs, p_corrected, reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq = 'TsC'\n",
    "# norm = 'length'\n",
    "norm = 'Ts'\n",
    "freq_LH,freq_LL,freq_HH = get_frequencies(seq,blocks_LH,blocks_LL,blocks_HH,normalize_by=norm)\n",
    "freqname = 'Frequency'\n",
    "groupname = 'Group'\n",
    "\n",
    "df0 = pd.DataFrame(freq_LH, columns=[freqname])\n",
    "df0[groupname] = 'LH'\n",
    "df1 = pd.DataFrame(freq_LL, columns=[freqname])\n",
    "df1[groupname] = 'LL'\n",
    "df2 = pd.DataFrame(freq_HH, columns=[freqname])\n",
    "df2[groupname] = 'HH'\n",
    "df_all = df0.append(df1).append(df2).reset_index()\n",
    "# df_all.boxplot('freq',by='group')\n",
    "\n",
    "ax = sns.boxplot(x=groupname, y=freqname, data=df_all)\n",
    "ax = sns.swarmplot(x=groupname, y=freqname, data=df_all, edgecolor=\"black\", linewidth=.9)\n",
    "\n",
    "# F, p = stats.kruskal(freq_LH,freq_LL,freq_HH)\n",
    "# print F, p\n",
    "print \"Comparing the median frequency usage of sequence {0} normalized by {1}\".format(seq, norm)\n",
    "H, p_omnibus, Z_pairs, p_corrected, reject = kw_dunn([freq_LH,freq_LL,freq_HH])\n",
    "print \"Kruskal omnibus test: H statistic = {0}, p<{1}\".format(H, p_omnibus)\n",
    "print \"Dunn's pairwise post hoc test with Bonferonni:\"\n",
    "print \"LH vs LL: Z score = {0}, p<{1}\".format(Z_pairs[0], p_corrected[0])\n",
    "print \"LH vs HH: Z score = {0}, p<{1}\".format(Z_pairs[1], p_corrected[1])\n",
    "print \"LL vs HH: Z score = {0}, p<{1}\".format(Z_pairs[2], p_corrected[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat sequence analysis ignoring the first time bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%aimport mining_functions\n",
    "\n",
    "BINS = 3\n",
    "axis = None\n",
    "tops_both_nofirsttimebin = get_top_seqs(rank_sequences(sequence_counts_both,BINS,axis,ignore_first_time_bin=True),15)\n",
    "\n",
    "axis = 0\n",
    "tops_time_nofirsttimebin = get_top_seqs(rank_sequences(sequence_counts_both,BINS,axis,ignore_first_time_bin=True),15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By group and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table = [['SEQ','IG','LH','LL','HH','t1','t2','t3','t4']]\n",
    "tops_both_nofirsttimebin.sort(key=lambda x: -x[1])\n",
    "for seq,ig in tops_both_nofirsttimebin:\n",
    "#     print seq, round(ig,3), np.ndarray.tolist(sequence_counts_both[seq].T*group_sizes)[0]\n",
    "    row = [seq,str(round(ig,2))] + [int(x*100) for x in np.sum(sequence_counts_both[seq][:,1:], axis=1)] +['-']+ [int(x*100) for x in np.sum((sequence_counts_both[seq][:,1:].T*group_sizes).T, axis=0)/sum(group_sizes)]\n",
    "    table.append(row)\n",
    "print tabulate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"New seqs from dropping first time bin:\",set(zip(*tops_both_nofirsttimebin)[0]) - set(zip(*tops_both)[0])\n",
    "print \"Lost seqs from dropping first time bin:\", set(zip(*tops_both)[0]) - set(zip(*tops_both_nofirsttimebin)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "all_seqs = zip(*tops_both_nofirsttimebin)[0]\n",
    "fig, axes = plt.subplots(len(tops_both_nofirsttimebin),1)#, sharex=True, sharey=True)\n",
    "maximum = max([np.amax(counts) for seq,counts in sequence_counts_both.iteritems() if seq in all_seqs])\n",
    "for i,(seq,ig) in enumerate(tops_both_nofirsttimebin):\n",
    "    ax = axes[i]\n",
    "    data = sequence_counts_both[seq]\n",
    "    hl,ll,hh = np.ndarray.tolist(sequence_counts_group[seq].T*group_sizes)[0]\n",
    "    seq = \"$\"+seq.replace('Ts','T_2').replace('Tb','T_1').replace('Tc','T_m')+\"$\"\n",
    "    ax.text(0,3.05, seq, fontsize=14)\n",
    "    ax.text(2.0,3.05, \"$IG$= {0}, $LH$: {1}, $LL$: {2}, $HH$: {3}\".format(str(round(ig,2)),hl,ll,hh), fontsize=10)\n",
    "    \n",
    "    heatmap = sns.heatmap(data, ax=ax, cmap=plt.cm.Blues,alpha=0.8, vmin=0, vmax=maximum, cbar=False,\n",
    "                          annot=True) #, annot_kws={'fontweight':'bold'})\n",
    "    fig.set_size_inches(10,40)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5)\n",
    "    ylabels = ['$HH$','$LL$','$LH$']\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    ax.set_xticklabels(['','','',''])\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"New seqs from dropping first time bin:\",set(zip(*tops_time_nofirsttimebin)[0]) - set(zip(*tops_time)[0])\n",
    "print \"Lost seqs from dropping first time bin:\", set(zip(*tops_time)[0]) - set(zip(*tops_time_nofirsttimebin)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "all_seqs = zip(*tops_time_nofirsttimebin)[0]\n",
    "fig, axes = plt.subplots(len(tops_time_nofirsttimebin),1)#, sharex=True, sharey=True)\n",
    "maximum = max([np.amax(counts) for seq,counts in sequence_counts_both.iteritems() if seq in all_seqs])\n",
    "for i,(seq,ig) in enumerate(tops_time_nofirsttimebin):\n",
    "    ax = axes[i]\n",
    "    data = sequence_counts_both[seq]\n",
    "    hl,ll,hh = np.ndarray.tolist(sequence_counts_group[seq].T*group_sizes)[0]\n",
    "    seq = \"$\"+seq.replace('Ts','T_2').replace('Tb','T_1').replace('Tc','T_m')+\"$\"\n",
    "    ax.text(0,3.05, seq, fontsize=14)\n",
    "    ax.text(2.0,3.05, \"$IG$= {0}, $LH$: {1}, $LL$: {2}, $HH$: {3}\".format(str(round(ig,2)),hl,ll,hh), fontsize=10)\n",
    "    \n",
    "    heatmap = sns.heatmap(data, ax=ax, cmap=plt.cm.Blues,alpha=0.8, vmin=0, vmax=maximum, cbar=False,\n",
    "                          annot=True) #, annot_kws={'fontweight':'bold'})\n",
    "    fig.set_size_inches(10,40)\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5)\n",
    "    ylabels = ['$HH$','$LL$','$LH$']\n",
    "    ax.set_yticklabels(ylabels)\n",
    "    ax.set_xticklabels(['','','',''])\n",
    "        \n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda2]",
   "language": "python",
   "name": "Python [Anaconda2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
